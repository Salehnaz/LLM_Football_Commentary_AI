# -*- coding: utf-8 -*-
"""CommentaryAI_Football_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LCI4JJ5STbX2CQU3LV8cbUACONHrayAC

# ‚öΩ CommentaryAI - Football Event Classifier
A simple transformer-based AI model that reads football commentary lines and classifies the event (Goal, Miss, Save, etc.).

üîç No traditional if-else coding ‚Äî just AI learning from text patterns!

---
"""

# üõ†Ô∏è Install dependencies
!pip install torch scikit-learn --quiet

import os
print(os.getcwd())

from google.colab import files

# Upload your CSV
uploaded = files.upload()

import pandas as pd

# Now read it
df = pd.read_csv("commentary_dataset_200.csv")
print(df.head())

# üì¶ Step 1: Prepare Sample Data
# samples = [
#     ("He shoots and scores!", "GOAL"),
#     ("That‚Äôs a brilliant save by the keeper!", "SAVE"),
#     ("He passes it to the wing.", "PASS"),
#     ("It‚Äôs wide of the post!", "MISS"),
#     ("He takes a long shot!", "SHOT"),
#     ("He beats the defender with a dribble.", "DRIBBLE"),
#     ("That‚Äôs a foul by the defender!", "FOUL"),
#     ("Header from the corner ‚Äî goal!", "GOAL"),
#     ("The striker completely misses the target.", "MISS"),
#     ("He calmly rolls it to the midfielder.", "PASS"),
# ]

import pandas as pd

# üì¶ Load data from uploaded CSV
df = pd.read_csv("commentary_dataset_200.csv")

# See a preview
print(df.head())

# Extract samples
samples = list(zip(df['commentary'], df['label']))

from google.colab import drive
drive.mount('/content/drive')

# üî† Step 2: Tokenization
token2idx = {"<PAD>": 0, "<UNK>": 1}
for sentence, _ in samples:
    for token in sentence.lower().split():
        if token not in token2idx:
            token2idx[token] = len(token2idx)

def encode_sentence(sentence, max_len=10):
    tokens = sentence.lower().split()
    token_ids = [token2idx.get(tok, token2idx["<UNK>"]) for tok in tokens]
    return token_ids[:max_len] + [0] * (max_len - len(token_ids))

X = [encode_sentence(s, max_len=10) for s, _ in samples]

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform([label for _, label in samples])

# üìö Step 3: Dataset & Transformer Model
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

class CommentaryDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X)
        self.y = torch.tensor(y)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx): return self.X[idx], self.y[idx]

class CommentaryAIModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=2)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)
        self.fc = nn.Linear(embed_dim, num_classes)
    def forward(self, x):
        x = self.embedding(x)
        x = x.permute(1, 0, 2)
        x = self.transformer(x)
        x = x.mean(dim=0)
        return self.fc(x)

# üß† Step 4: Training the model
dataset = CommentaryDataset(X, y)
loader = DataLoader(dataset, batch_size=2, shuffle=True)
model = CommentaryAIModel(vocab_size=len(token2idx), embed_dim=32, num_classes=len(le.classes_))
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

for epoch in range(100):
    total_loss, correct = 0, 0
    for xb, yb in loader:
        preds = model(xb)
        loss = loss_fn(preds, yb)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        total_loss += loss.item()
        correct += (preds.argmax(dim=1) == yb).sum().item()
    print(f"üìò Epoch {epoch+1} - Loss: {total_loss:.4f} - Accuracy: {correct}/{len(dataset)}")

# üß™ Step 5: Inference (No if-else!)
def predict_commentary(sentence):
    model.eval()
    with torch.no_grad():
        encoded = torch.tensor([encode_sentence(sentence, max_len=10)])
        output = model(encoded)
        pred = torch.argmax(output, dim=1).item()
        return le.inverse_transform([pred])[0]

# üîç Try with new sentences
test_lines = [
    "He smashes it into the net!",
    "The ball is passed across the field.",
    "Great dribble from midfield!",
    "Misses the chance to equalize.",
    "That tackle might be a foul.",
]

print("\nüß† Predictions:")
for line in test_lines:
    print(f"{line} ‚Üí {predict_commentary(line)}")